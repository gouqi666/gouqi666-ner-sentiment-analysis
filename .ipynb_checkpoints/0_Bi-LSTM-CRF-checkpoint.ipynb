{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>BIO_anno</th>\n",
       "      <th>class</th>\n",
       "      <th>bank_topic</th>\n",
       "      <th>training_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...</td>\n",
       "      <td>[B-BANK, I-BANK, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>([交, 行, 1, 4, 年, 用, 过, ，, 半, 年, 准, 备, 提, 额, ，,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>单标我有了，最近visa双标返现活动好</td>\n",
       "      <td>[B-PRODUCT, I-PRODUCT, O, O, O, O, O, O, B-PRO...</td>\n",
       "      <td>1</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>([单, 标, 我, 有, 了, ，, 最, 近, v, i, s, a, 双, 标, 返,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>建设银行提额很慢的……</td>\n",
       "      <td>[B-BANK, I-BANK, I-BANK, I-BANK, B-COMMENTS_N,...</td>\n",
       "      <td>0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>([建, 设, 银, 行, 提, 额, 很, 慢, 的, …, …], [B-BANK, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>孙女士在原恒泰农村合作银行存入50万元，同年9月又存款50万元。2014年，恒泰农村合作银行...</td>\n",
       "      <td>[O, O, O, O, O, B-BANK, I-BANK, I-BANK, I-BANK...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>([孙, 女, 士, 在, 原, 恒, 泰, 农, 村, 合, 作, 银, 行, 存, 入,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I...</td>\n",
       "      <td>2</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>([我, 的, 怎, 么, 显, 示, 0, ., 2, 5, 费, 率, ，, 而, 且,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>中信就喜欢你1000以上的刷，刷多了还会说你存在恶意套现封卡降额，笑死人</td>\n",
       "      <td>[B-BANK, I-BANK, O, B-COMMENTS_ADJ, I-COMMENTS...</td>\n",
       "      <td>2</td>\n",
       "      <td>中信银行</td>\n",
       "      <td>([中, 信, 就, 喜, 欢, 你, 1, 0, 0, 0, 以, 上, 的, 刷, ，,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>好像没必要养中信</td>\n",
       "      <td>[O, O, O, O, O, O, B-BANK, I-BANK]</td>\n",
       "      <td>2</td>\n",
       "      <td>中信银行</td>\n",
       "      <td>([好, 像, 没, 必, 要, 养, 中, 信], [O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>像我付了五毛钱给我冻结以后销户都不行。</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I-CO...</td>\n",
       "      <td>2</td>\n",
       "      <td>中信银行</td>\n",
       "      <td>([像, 我, 付, 了, 五, 毛, 钱, 给, 我, 冻, 结, 以, 后, 销, 户,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>结算卡吗</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "      <td>2</td>\n",
       "      <td>中信银行</td>\n",
       "      <td>([结, 算, 卡, 吗], [O, O, O, O])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>可以。像我付了五毛钱给我冻结以后销户都不行。</td>\n",
       "      <td>[B-COMMENTS_ADJ, I-COMMENTS_ADJ, O, O, O, O, O...</td>\n",
       "      <td>2</td>\n",
       "      <td>中信银行</td>\n",
       "      <td>([可, 以, 。, 像, 我, 付, 了, 五, 毛, 钱, 给, 我, 冻, 结, 以,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...   \n",
       "1                                   单标我有了，最近visa双标返现活动好   \n",
       "2                                           建设银行提额很慢的……   \n",
       "3     孙女士在原恒泰农村合作银行存入50万元，同年9月又存款50万元。2014年，恒泰农村合作银行...   \n",
       "4                    我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k   \n",
       "...                                                 ...   \n",
       "9995               中信就喜欢你1000以上的刷，刷多了还会说你存在恶意套现封卡降额，笑死人   \n",
       "9996                                           好像没必要养中信   \n",
       "9997                                像我付了五毛钱给我冻结以后销户都不行。   \n",
       "9998                                               结算卡吗   \n",
       "9999                             可以。像我付了五毛钱给我冻结以后销户都不行。   \n",
       "\n",
       "                                               BIO_anno  class bank_topic  \\\n",
       "0     [B-BANK, I-BANK, O, O, O, O, O, O, O, O, O, O,...      0       建设银行   \n",
       "1     [B-PRODUCT, I-PRODUCT, O, O, O, O, O, O, B-PRO...      1       建设银行   \n",
       "2     [B-BANK, I-BANK, I-BANK, I-BANK, B-COMMENTS_N,...      0       建设银行   \n",
       "3     [O, O, O, O, O, B-BANK, I-BANK, I-BANK, I-BANK...      2        NaN   \n",
       "4     [O, O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I...      2       建设银行   \n",
       "...                                                 ...    ...        ...   \n",
       "9995  [B-BANK, I-BANK, O, B-COMMENTS_ADJ, I-COMMENTS...      2       中信银行   \n",
       "9996                 [O, O, O, O, O, O, B-BANK, I-BANK]      2       中信银行   \n",
       "9997  [O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I-CO...      2       中信银行   \n",
       "9998                                       [O, O, O, O]      2       中信银行   \n",
       "9999  [B-COMMENTS_ADJ, I-COMMENTS_ADJ, O, O, O, O, O...      2       中信银行   \n",
       "\n",
       "                                          training_data  \n",
       "0     ([交, 行, 1, 4, 年, 用, 过, ，, 半, 年, 准, 备, 提, 额, ，,...  \n",
       "1     ([单, 标, 我, 有, 了, ，, 最, 近, v, i, s, a, 双, 标, 返,...  \n",
       "2     ([建, 设, 银, 行, 提, 额, 很, 慢, 的, …, …], [B-BANK, I...  \n",
       "3     ([孙, 女, 士, 在, 原, 恒, 泰, 农, 村, 合, 作, 银, 行, 存, 入,...  \n",
       "4     ([我, 的, 怎, 么, 显, 示, 0, ., 2, 5, 费, 率, ，, 而, 且,...  \n",
       "...                                                 ...  \n",
       "9995  ([中, 信, 就, 喜, 欢, 你, 1, 0, 0, 0, 以, 上, 的, 刷, ，,...  \n",
       "9996  ([好, 像, 没, 必, 要, 养, 中, 信], [O, O, O, O, O, O, ...  \n",
       "9997  ([像, 我, 付, 了, 五, 毛, 钱, 给, 我, 冻, 结, 以, 后, 销, 户,...  \n",
       "9998                       ([结, 算, 卡, 吗], [O, O, O, O])  \n",
       "9999  ([可, 以, 。, 像, 我, 付, 了, 五, 毛, 钱, 给, 我, 冻, 结, 以,...  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "train_data = pd.read_csv('./train_data_public.csv')\n",
    "train_data.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "test_data = pd.read_csv('./test_public.csv')\n",
    "\n",
    "train_data['BIO_anno'] = train_data['BIO_anno'].apply(lambda x:x.split(' '))\n",
    "train_data['training_data'] = train_data.apply(lambda row: (list(row['text']), row['BIO_anno']), axis = 1)\n",
    "test_data['testing_data'] = test_data.apply(lambda row: (list(row['text'])), axis = 1)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_txt = []\n",
    "testing_data_txt = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    training_data_txt.append(train_data.iloc[i]['training_data'])\n",
    "    \n",
    "for i in range(len(test_data)):\n",
    "    testing_data_txt.append(test_data.iloc[i]['testing_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM Conditional Random Field\n",
    "### pytorch tutorials https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]\n",
      "Torch version:  1.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "print(\"Python version:  %s\"%(sys.version))\n",
    "print (\"Torch version:  %s\"%(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x197ed36f8d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务1：实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    # 返回vec的dim为1维度上的最大值索引\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    # 将句子转化为ID\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "# 前向算法是不断累积之前的结果，这样就会有个缺点\n",
    "# 指数和累积到一定程度后，会超过计算机浮点值的最大值，变成inf，这样取log后也是inf\n",
    "# 为了避免这种情况，用一个合适的值clip去提指数和的公因子，这样就不会使某项变得过大而无法计算\n",
    "# SUM = log(exp(s1)+exp(s2)+...+exp(s100))\n",
    "#     = log{exp(clip)*[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]}\n",
    "#     = clip + log[exp(s1-clip)+exp(s2-clip)+...+exp(s100-clip)]\n",
    "# where clip=max\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim    # word embedding dim\n",
    "        self.hidden_dim = hidden_dim          # Bi-LSTM hidden dim\n",
    "        self.vocab_size = vocab_size          \n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # 将BiLSTM提取的特征向量映射到特征空间，即经过全连接得到发射分数\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # 转移矩阵的参数初始化，transitions[i,j]代表的是从第j个tag转移到第i个tag的转移分数\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # 初始化所有其他tag转移到START_TAG的分数非常小，即不可能由其他tag转移到START_TAG\n",
    "        # 初始化STOP_TAG转移到所有其他tag的分数非常小，即不可能由STOP_TAG转移到其他tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # 初始化LSTM的参数\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "    \n",
    "    def _get_lstm_features(self, sentence):\n",
    "        # 通过Bi-LSTM提取特征\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "    \n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # 计算给定tag序列的分数，即一条路径的分数\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            # 递推计算路径分数：转移分数 + 发射分数\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # 通过前向算法递推计算\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # 初始化step 0即START位置的发射分数，START_TAG取0其他位置取-10000\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # 将初始化START位置为0的发射分数赋值给previous\n",
    "        previous = init_alphas\n",
    "\n",
    "        # 迭代整个句子\n",
    "        for obs in feats:\n",
    "            # 当前时间步的前向tensor\n",
    "            alphas_t = []\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 取出当前tag的发射分数，与之前时间步的tag无关\n",
    "                emit_score = obs[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # 取出当前tag由之前tag转移过来的转移分数\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # 当前路径的分数：之前时间步分数 + 转移分数 + 发射分数\n",
    "                next_tag_var = previous + trans_score + emit_score\n",
    "                # 对当前分数取log-sum-exp\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            # 更新previous 递推计算下一个时间步\n",
    "            previous = torch.cat(alphas_t).view(1, -1)\n",
    "        # 考虑最终转移到STOP_TAG\n",
    "        terminal_var = previous + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        # 计算最终的分数\n",
    "        scores = log_sum_exp(terminal_var)\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # 初始化viterbi的previous变量\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        previous = init_vvars\n",
    "        for obs in feats:\n",
    "            # 保存当前时间步的回溯指针\n",
    "            bptrs_t = []\n",
    "            # 保存当前时间步的viterbi变量\n",
    "            viterbivars_t = []  \n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # 维特比算法记录最优路径时只考虑上一步的分数以及上一步tag转移到当前tag的转移分数\n",
    "                # 并不取决与当前tag的发射分数\n",
    "                next_tag_var = previous + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # 更新previous，加上当前tag的发射分数obs\n",
    "            previous = (torch.cat(viterbivars_t) + obs).view(1, -1)\n",
    "            # 回溯指针记录当前时间步各个tag来源前一步的tag\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        # 考虑转移到STOP_TAG的转移分数\n",
    "        terminal_var = previous + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # 通过回溯指针解码出最优路径\n",
    "        best_path = [best_tag_id]\n",
    "        # best_tag_id作为线头，反向遍历backpointers找到最优路径\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # 去除START_TAG\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        # CRF损失函数由两部分组成，真实路径的分数和所有路径的总分数。\n",
    "        # 真实路径的分数应该是所有路径中分数最高的。\n",
    "        # log真实路径的分数/log所有可能路径的分数，越大越好，构造crf loss函数取反，loss越小越好\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # 通过BiLSTM提取发射分数\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # 根据发射分数以及转移分数，通过viterbi解码找到一条最优路径\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(147.7163), [8, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 3, 4, 0, 3, 4, 0, 8, 3, 4, 0, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 3, 4, 0, 8, 3, 4, 0, 8, 3, 4, 0, 5])\n",
      "the 0  epoch\n",
      "Time Taken: 0 seconds\n",
      "the 1  epoch\n",
      "Time Taken: 9 seconds\n",
      "the 2  epoch\n",
      "Time Taken: 17 seconds\n",
      "the 3  epoch\n",
      "Time Taken: 24 seconds\n",
      "the 4  epoch\n",
      "Time Taken: 32 seconds\n",
      "the 5  epoch\n",
      "Time Taken: 39 seconds\n",
      "the 6  epoch\n",
      "Time Taken: 47 seconds\n",
      "the 7  epoch\n",
      "Time Taken: 54 seconds\n",
      "the 8  epoch\n",
      "Time Taken: 61 seconds\n",
      "the 9  epoch\n",
      "Time Taken: 70 seconds\n",
      "(tensor(222.6020), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 11\n",
    "HIDDEN_DIM = 6\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "# 将训练集汉字使用数字表示\n",
    "# 为了方便调试，先使用100条数据进行模型训练，选手可以采用全量数据进行训练\n",
    "training_data = training_data_txt[:100] \n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "# 将测试集汉字使用数字表示\n",
    "testing_data = testing_data_txt\n",
    "for sentence in testing_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = { \"O\": 0, \"B-BANK\": 1, \"I-BANK\": 2, \"B-PRODUCT\":3,'I-PRODUCT':4, \n",
    "             'B-COMMENTS_N':5, 'I-COMMENTS_N':6, 'B-COMMENTS_ADJ':7, \n",
    "             'I-COMMENTS_ADJ':8, START_TAG: 9, STOP_TAG: 10}\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# 训练前检查模型预测结果\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    print(model(precheck_sent))\n",
    "    a = model(precheck_sent)\n",
    "    a = pd.Series(a)\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(10): \n",
    "    print('the',epoch,' epoch')\n",
    "    print(f'Time Taken: {round(time.time()-t)} seconds')\n",
    "    for sentence, tags in training_data:\n",
    "        # 第一步，pytorch梯度累积，需要清零梯度\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 第二步，将输入转化为tensors\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # 进行前向计算，取出crf loss\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # 第四步，计算loss，梯度，通过optimier更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 训练结束查看模型预测结果，对比观察模型是否学到\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[3][0], word_to_ix)\n",
    "    print(model(precheck_sent))\n",
    "    a = model(precheck_sent)\n",
    "    a = pd.Series(a)\n",
    "    a.to_csv('test1.csv')\n",
    "# We got it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任务2：情感分类\n",
    "\n",
    "1. 使用TF-IDF向量化\n",
    "2. 线性回归进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(train_data['text'])\n",
    "X_test = list(train_data['text'][800:1000])\n",
    "\n",
    "y_train = list(train_data['class'])\n",
    "y_test = list(train_data['class'][800:1000])\n",
    "\n",
    "test_data_sent = list(test_data['text']) # 列表格式\n",
    "text_all = X_train + test_data_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectoriser fitted.\n",
      "No. of feature_words:  75936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(text_all)\n",
    "print(f'Vectoriser fitted.')\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Transformed.\n"
     ]
    }
   ],
   "source": [
    "X_train = vectoriser.transform(X_train)\n",
    "test_data_text  = vectoriser.transform(test_data_sent)\n",
    "# X_test  = vectoriser.transform(X_test)\n",
    "print(f'Data Transformed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2, max_iter=1000, n_jobs=-1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('vectoriser-ngram-(1,2).pickle','wb')\n",
    "pickle.dump(vectoriser, file)\n",
    "file.close()\n",
    "\n",
    "file = open('Sentiment-LR.pickle','wb')\n",
    "pickle.dump(LRmodel, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>你曲线过了？</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我以前joy下卡1w曲线芭比白到2w。你都2w了曲线只能是大山白了。别的卡没有曲线这一说了</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>不知道啊，我是民生银行啊</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>大白免费有</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>说不准的，主要看你个人资质，有人邀请办卡额度提升，也有人额度无变化。不介意多张卡的话可以试试</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>楼主6个月查询次数几条？靠谱么</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>不错不错我只买了1w3左右但愿也能早点刷出早日批卡</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>这额度用来买菜吗</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>07009是什么东西，完全搞不懂</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>好羡慕有卡的人</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text  class\n",
       "0                                            你曲线过了？      2\n",
       "1     我以前joy下卡1w曲线芭比白到2w。你都2w了曲线只能是大山白了。别的卡没有曲线这一说了      2\n",
       "2                                      不知道啊，我是民生银行啊      2\n",
       "3                                             大白免费有      2\n",
       "4    说不准的，主要看你个人资质，有人邀请办卡额度提升，也有人额度无变化。不介意多张卡的话可以试试      2\n",
       "..                                              ...    ...\n",
       "195                                 楼主6个月查询次数几条？靠谱么      2\n",
       "196                       不错不错我只买了1w3左右但愿也能早点刷出早日批卡      2\n",
       "197                                        这额度用来买菜吗      2\n",
       "198                                07009是什么东西，完全搞不懂      2\n",
       "199                                         好羡慕有卡的人      2\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_models():\n",
    "    '''\n",
    "    Replace '..path/' by the path of the saved models.\n",
    "    '''\n",
    "    \n",
    "    # Load the vectoriser.\n",
    "    file = open('./vectoriser-ngram-(1,2).pickle', 'rb')\n",
    "    vectoriser = pickle.load(file)\n",
    "    file.close()\n",
    "    # Load the LR Model.\n",
    "    file = open('./Sentiment-LR.pickle', 'rb')\n",
    "    LRmodel = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return vectoriser, LRmodel\n",
    "\n",
    "def predict(vectoriser, model, text):\n",
    "    # Predict the sentiment\n",
    "    textdata = vectoriser.transform(text)\n",
    "    sentiment = model.predict(textdata)\n",
    "    \n",
    "    # Make a list of text with sentiment.\n",
    "    data = []\n",
    "    for text, pred in zip(text, sentiment):\n",
    "        data.append((text,pred))\n",
    "        \n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(data, columns = ['text','class'])\n",
    "    return df\n",
    "\n",
    "sentiment_result = predict(vectoriser, LRmodel, X_test)\n",
    "sentiment_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>一个月有不到40笔吗？卡多了也是累赘。再看就不能打电话交保护费保险了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>光大│福卡13K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>对呀，才四万，又不是很多</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>为啥我10万每个月800啊，这个额度分分钟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3. 债券，一般有多余的钱可以考虑的，还是不错的投资</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>5088</td>\n",
       "      <td>不能超过信用额度，没多大意思</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>5089</td>\n",
       "      <td>联系就联系呗，是银行天天让分期好烦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>5090</td>\n",
       "      <td>和你一样卡，所以我在想怎么换卡或者继续用老磁条卡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>5091</td>\n",
       "      <td>我也奇怪，广发明明知道我一下办这么多卡就是撸毛，怎么一张没拒绝我？我对后面两张卡就没办希望，</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>5092</td>\n",
       "      <td>哦，我不能攻击我们的公积金。在公司内部买的意思是</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5093 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                            text\n",
       "0        0             一个月有不到40笔吗？卡多了也是累赘。再看就不能打电话交保护费保险了。\n",
       "1        1                                        光大│福卡13K\n",
       "2        2                                    对呀，才四万，又不是很多\n",
       "3        3                           为啥我10万每个月800啊，这个额度分分钟\n",
       "4        4                      3. 债券，一般有多余的钱可以考虑的，还是不错的投资\n",
       "...    ...                                             ...\n",
       "5088  5088                                  不能超过信用额度，没多大意思\n",
       "5089  5089                               联系就联系呗，是银行天天让分期好烦\n",
       "5090  5090                        和你一样卡，所以我在想怎么换卡或者继续用老磁条卡\n",
       "5091  5091  我也奇怪，广发明明知道我一下办这么多卡就是撸毛，怎么一张没拒绝我？我对后面两张卡就没办希望，\n",
       "5092  5092                        哦，我不能攻击我们的公积金。在公司内部买的意思是\n",
       "\n",
       "[5093 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('./test_public.csv')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_tag = dict([v,k] for k, v in tag_to_ix.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在test数据集预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实体识别结果\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_data)):\n",
    "        precheck_sent = prepare_sequence(test_data.iloc[i][1], word_to_ix)\n",
    "        sig_res = model(precheck_sent)[1]\n",
    "        for i in range(len(sig_res)):\n",
    "            sig_res[i] = ix_to_tag[sig_res[i]]\n",
    "        result.append(' '.join(sig_res))\n",
    "test_data['BIO_anno'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O',\n",
       " 'O O O O O O O O',\n",
       " 'O O O O O O O O O O O O',\n",
       " 'O O O O O O O O O O O O O O O O O O O O O',\n",
       " 'O O O O O O O O O O O O O O O O O O O O O O O O O O',\n",
       " 'O O O O O O O O O O O',\n",
       " 'O O O O O O O O O O O O O O O O O',\n",
       " 'O O O O O O O O O O O O O O O O O O O O O',\n",
       " 'O O O O O O O O O O O O O O O O O',\n",
       " 'O O']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测test data 的sentiment 分类\n",
    "sentiment_result = predict(vectoriser, LRmodel, test_data_sent)\n",
    "test_data['class'] = list(sentiment_result['class'])\n",
    "# test_data.to_csv('test_baseline.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
