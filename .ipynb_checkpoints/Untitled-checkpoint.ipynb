{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "75001ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>BIO_anno</th>\n",
       "      <th>class</th>\n",
       "      <th>training_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...</td>\n",
       "      <td>[B-BANK, I-BANK, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>0</td>\n",
       "      <td>([交, 行, 1, 4, 年, 用, 过, ，, 半, 年, 准, 备, 提, 额, ，,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>单标我有了，最近visa双标返现活动好</td>\n",
       "      <td>[B-PRODUCT, I-PRODUCT, O, O, O, O, O, O, B-PRO...</td>\n",
       "      <td>1</td>\n",
       "      <td>([单, 标, 我, 有, 了, ，, 最, 近, v, i, s, a, 双, 标, 返,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>建设银行提额很慢的……</td>\n",
       "      <td>[B-BANK, I-BANK, I-BANK, I-BANK, B-COMMENTS_N,...</td>\n",
       "      <td>0</td>\n",
       "      <td>([建, 设, 银, 行, 提, 额, 很, 慢, 的, …, …], [B-BANK, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I...</td>\n",
       "      <td>2</td>\n",
       "      <td>([我, 的, 怎, 么, 显, 示, 0, ., 2, 5, 费, 率, ，, 而, 且,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>利率不错，可以撸</td>\n",
       "      <td>[B-COMMENTS_N, I-COMMENTS_N, B-COMMENTS_ADJ, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>([利, 率, 不, 错, ，, 可, 以, 撸], [B-COMMENTS_N, I-CO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>7523</td>\n",
       "      <td>我鼎级拒了</td>\n",
       "      <td>[O, O, O, B-COMMENTS_ADJ, O]</td>\n",
       "      <td>2</td>\n",
       "      <td>([我, 鼎, 级, 拒, 了], [O, O, O, B-COMMENTS_ADJ, O])</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>7524</td>\n",
       "      <td>一打一个准，准胜，看激活信用卡时那协议，全是对银行有利的</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I...</td>\n",
       "      <td>2</td>\n",
       "      <td>([一, 打, 一, 个, 准, ，, 准, 胜, ，, 看, 激, 活, 信, 用, 卡,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>7525</td>\n",
       "      <td>招行分期白80k</td>\n",
       "      <td>[B-BANK, I-BANK, B-PRODUCT, I-PRODUCT, I-PRODU...</td>\n",
       "      <td>2</td>\n",
       "      <td>([招, 行, 分, 期, 白, 8, 0, k], [B-BANK, I-BANK, B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>7526</td>\n",
       "      <td>5万，额度还行吧没毕业哦</td>\n",
       "      <td>[O, O, O, B-COMMENTS_N, I-COMMENTS_N, O, O, O,...</td>\n",
       "      <td>2</td>\n",
       "      <td>([5, 万, ，, 额, 度, 还, 行, 吧, 没, 毕, 业, 哦], [O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>7527</td>\n",
       "      <td>张家港农商、江阴农商、无锡农商试试</td>\n",
       "      <td>[B-BANK, I-BANK, I-BANK, I-BANK, I-BANK, O, B-...</td>\n",
       "      <td>2</td>\n",
       "      <td>([张, 家, 港, 农, 商, 、, 江, 阴, 农, 商, 、, 无, 锡, 农, 商,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7528 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0        0  交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...   \n",
       "1        1                                单标我有了，最近visa双标返现活动好   \n",
       "2        2                                        建设银行提额很慢的……   \n",
       "3        3                 我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k   \n",
       "4        4                                           利率不错，可以撸   \n",
       "...    ...                                                ...   \n",
       "7523  7523                                              我鼎级拒了   \n",
       "7524  7524                       一打一个准，准胜，看激活信用卡时那协议，全是对银行有利的   \n",
       "7525  7525                                           招行分期白80k   \n",
       "7526  7526                                       5万，额度还行吧没毕业哦   \n",
       "7527  7527                                  张家港农商、江阴农商、无锡农商试试   \n",
       "\n",
       "                                               BIO_anno  class  \\\n",
       "0     [B-BANK, I-BANK, O, O, O, O, O, O, O, O, O, O,...      0   \n",
       "1     [B-PRODUCT, I-PRODUCT, O, O, O, O, O, O, B-PRO...      1   \n",
       "2     [B-BANK, I-BANK, I-BANK, I-BANK, B-COMMENTS_N,...      0   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I...      2   \n",
       "4     [B-COMMENTS_N, I-COMMENTS_N, B-COMMENTS_ADJ, I...      1   \n",
       "...                                                 ...    ...   \n",
       "7523                       [O, O, O, B-COMMENTS_ADJ, O]      2   \n",
       "7524  [O, O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I...      2   \n",
       "7525  [B-BANK, I-BANK, B-PRODUCT, I-PRODUCT, I-PRODU...      2   \n",
       "7526  [O, O, O, B-COMMENTS_N, I-COMMENTS_N, O, O, O,...      2   \n",
       "7527  [B-BANK, I-BANK, I-BANK, I-BANK, I-BANK, O, B-...      2   \n",
       "\n",
       "                                          training_data  \n",
       "0     ([交, 行, 1, 4, 年, 用, 过, ，, 半, 年, 准, 备, 提, 额, ，,...  \n",
       "1     ([单, 标, 我, 有, 了, ，, 最, 近, v, i, s, a, 双, 标, 返,...  \n",
       "2     ([建, 设, 银, 行, 提, 额, 很, 慢, 的, …, …], [B-BANK, I...  \n",
       "3     ([我, 的, 怎, 么, 显, 示, 0, ., 2, 5, 费, 率, ，, 而, 且,...  \n",
       "4     ([利, 率, 不, 错, ，, 可, 以, 撸], [B-COMMENTS_N, I-CO...  \n",
       "...                                                 ...  \n",
       "7523    ([我, 鼎, 级, 拒, 了], [O, O, O, B-COMMENTS_ADJ, O])  \n",
       "7524  ([一, 打, 一, 个, 准, ，, 准, 胜, ，, 看, 激, 活, 信, 用, 卡,...  \n",
       "7525  ([招, 行, 分, 期, 白, 8, 0, k], [B-BANK, I-BANK, B-...  \n",
       "7526  ([5, 万, ，, 额, 度, 还, 行, 吧, 没, 毕, 业, 哦], [O, O, ...  \n",
       "7527  ([张, 家, 港, 农, 商, 、, 江, 阴, 农, 商, 、, 无, 锡, 农, 商,...  \n",
       "\n",
       "[7528 rows x 5 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "train_data = pd.read_csv('./train_data_public.csv')\n",
    "# train_data.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "test_data = pd.read_csv('./test_public.csv')\n",
    "\n",
    "train_data['BIO_anno'] = train_data['BIO_anno'].apply(lambda x:x.split(' '))\n",
    "train_data['training_data'] = train_data.apply(lambda row: (list(row['text']), row['BIO_anno']), axis = 1)\n",
    "test_data['testing_data'] = test_data.apply(lambda row: (list(row['text'])), axis = 1)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a68266f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49b1fe837cd443e9f00a12caa6c2e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4f8feeea4f4189803e03a4e20dbd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b628f50e9704827808cbc587acd7f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231a7cfd521b454bb7e1906695b2021f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135fa3b98ce6418caf89d6cbeac253fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bertmodel = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "80a78a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class simple_model(torch.nn.Module):\n",
    "    def __init__(self,bertmodel,dropout=None):\n",
    "        super(simple_model,self).__init__()\n",
    "        self.bertmodel = bertmodel\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)\n",
    "        # 情感分类任务，0，1，2\n",
    "        self.classifier = nn.Linear(self.bertmodel.config.hidden_size, 3)\n",
    "    def forward(self,input_ids,token_type_ids,attention_mask):\n",
    "        outputs= self.bertmodel(input_ids,token_type_ids = token_type_ids, attention_mask=attention_mask)#\n",
    "        outputs = outputs.pooler_output\n",
    "        outputs = self.dropout(outputs)\n",
    "        # \n",
    "        logits = self.classifier(outputs)\n",
    "        return logits\n",
    "model = simple_model(bertmodel)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0677a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self,data,label=None,is_train = True):\n",
    "        super(myDataset).__init__()\n",
    "        self.input_ids = []\n",
    "        self.token_type_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.label = label\n",
    "        self.is_train = is_train\n",
    "        for text in data:\n",
    "            encoder_inputs = tokenizer(text,max_length = 128)\n",
    "            self.input_ids.append(encoder_inputs['input_ids'])\n",
    "            self.token_type_ids.append(encoder_inputs['token_type_ids'])\n",
    "            self.attention_mask.append(encoder_inputs['attention_mask'])\n",
    "    def __getitem__(self,idx):\n",
    "        input_ids = self.input_ids[idx]\n",
    "        token_type_ids = self.token_type_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "        label = self.label\n",
    "        if self.is_train:\n",
    "            return input_ids,token_type_ids,attention_mask,label[idx]\n",
    "        else:\n",
    "            return input_ids, token_type_ids,attention_mask,[1]\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "# print(len(train_data)) # train_data 总共7528条数据\n",
    "train_dataset = myDataset(train_data['text'],train_data['class'])\n",
    "train_dataset,valid_dataset = random_split(train_dataset,[6528,1000],generator=torch.Generator().manual_seed(42))\n",
    "test_dataset =  myDataset(test_data['text'],is_train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8a53f1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 100, 1740, 1872, 25108, 1989, 100, 1752, 1872, 11910, 1989, 100, 1741, 1872, 8698, 2692, 1986, 100, 1872, 100, 100, 2753, 100, 1989, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 2)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "06572495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(batch_data,pad=0):\n",
    "    inputs_ids,token_type_ids,attention_mask,label = list(zip(*(batch_data)))#batch_data的结构是[([texta_1],[textb_1],[label_1]),([texta_2],[textb_2],[label_2])，...]，所以需要使用zip函数对它解压\n",
    "    max_len = max([len(seq_a) for seq_a in inputs_ids]) #这里我使用的是一个batch中text_a或者是text_b的最大长度作为max_len,也可以自定义长度\n",
    "    inputs_ids = [seq + [pad]*(max_len-len(seq)) for seq in inputs_ids]\n",
    "    token_type_ids = [seq + [pad]*(max_len-len(seq)) for seq in token_type_ids]\n",
    "    attention_mask = [seq + [pad]*(max_len-len(seq)) for seq in attention_mask]\n",
    "    return torch.tensor(inputs_ids,dtype = torch.long),torch.tensor(token_type_ids,dtype = torch.long),torch.tensor(attention_mask,dtype = torch.long),torch.tensor(label,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8934f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_data_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size = 32,\n",
    "                collate_fn = my_collate_fn,\n",
    "                shuffle=True\n",
    ")\n",
    "valid_data_loader = DataLoader(\n",
    "                valid_dataset,\n",
    "                batch_size = 32,\n",
    "                collate_fn = my_collate_fn,\n",
    "                shuffle=True\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size = 32,\n",
    "                collate_fn = my_collate_fn,\n",
    "                shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1b5c4d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  101,   100,  1842,  ...,     0,     0,     0],\n",
      "        [  101,   100,   100,  ...,     0,     0,     0],\n",
      "        [  101,   100,   100,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,   100,   100,  ...,     0,     0,     0],\n",
      "        [  101,   100,  1840,  ...,     0,     0,     0],\n",
      "        [  101, 11910,   100,  ...,     0,     0,     0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), tensor([2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0,\n",
      "        0, 1, 2, 2, 2, 2, 0, 2]))\n"
     ]
    }
   ],
   "source": [
    "for batch in train_data_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "16dc16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer,milestones=[20,80],gamma = 0.9)\n",
    "epochs = 5\n",
    "# !pip install torchmetrics\n",
    "import torchmetrics as metrics\n",
    "train_accuracy = metrics.Accuracy()\n",
    "train_accuracy.to(device)\n",
    "ckpt_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9fe95b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 30, epoch: 1, batch: 30, loss: 0.51926, accu: 0.84375, speed: 0.04 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.71015, accu: 0.78125, speed: 0.05 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.41378, accu: 0.90625, speed: 0.05 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.66094, accu: 0.78125, speed: 0.04 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.45129, accu: 0.87500, speed: 0.05 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.48144, accu: 0.84375, speed: 0.05 step/s\n",
      "epoch:1,total_loss:0.51294,accu:0.85218\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-239-a33b28efaf4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# 评估当前训练的模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0msave_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_%d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#train\n",
    "epoches = 10\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "for epoch in range(1,epoches + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids,token_type_ids,attention_mask,labels = batch\n",
    "        input_ids = input_ids.to(device,dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device,dtype=torch.long)\n",
    "        attention_mask =attention_mask.to(device,dtype=torch.long)\n",
    "        labels = labels.to(device,dtype=torch.long)\n",
    "        # 喂数据给model\n",
    "        probs = model(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n",
    "        # 计算损失函数值\n",
    "        loss = criterion(probs, labels)\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # 预测分类概率值\n",
    "        # 计算acc\n",
    "        batch_acc = train_accuracy(probs, labels)\n",
    "        global_step += 1\n",
    "        if global_step % 30 == 0:\n",
    "#             acc = train_accuracy.compute()\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, batch_acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "    print('epoch:%d,total_loss:%.5f,accu:%.5f' %(epoch,train_loss * epoch / global_step, train_accuracy.compute()))\n",
    "    \n",
    "    # 评估当前训练的模型  \n",
    "    model.eval()\n",
    "    save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir) \n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "        valid_accuracy = metrics.Accuracy()\n",
    "        valid_accuracy.to(device)\n",
    "        for  i, batch in enumerate(valid_data_loader):\n",
    "            input_ids,token_type_ids,attention_mask,labels = batch\n",
    "            input_ids = input_ids.to(device,dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device,dtype=torch.long)\n",
    "            attention_mask =attention_mask.to(device,dtype=torch.long)\n",
    "            labels = labels.to(device,dtype=torch.long)\n",
    "            probs = model(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n",
    "            valid_accuracy(probs,labels)\n",
    "        acc = valid_accuracy.compute()\n",
    "        print('eval_acc: %.5f ' % acc)\n",
    "        torch.save(model.state_dict(),os.path.join(save_dir,'model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sentiment_result = []\n",
    "for step,batch in enumerate(test_data_loader):\n",
    "    input_ids,token_type_ids,attention_mask,_ = batch\n",
    "    input_ids = input_ids.to(device,dtype=torch.long)\n",
    "    token_type_ids = token_type_ids.to(device,dtype=torch.long)\n",
    "    attention_mask =attention_mask.to(device,dtype=torch.long)\n",
    "    probs = model(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n",
    "    sentiment_result.append(torch.argmax(probs))\n",
    "test_data['class'] = sentiment_result\n",
    "tese_data.to_csv('my_baseline.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
